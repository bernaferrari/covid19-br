{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# url = \"https://data.brasil.io/dataset/covid19/caso_full.csv.gz\"\n",
    "# with open(\"caso_full.csv.gz\", \"wb\") as f:\n",
    "#     r = requests.get(url)\n",
    "#     f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gzip\n",
    "# import shutil\n",
    "# with gzip.open('caso_full.csv.gz', 'rb') as f_in:\n",
    "#     with open('caso_full.csv', 'wb') as f_out:\n",
    "#         shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                 city  city_ibge_code        date  epidemiological_week  \\\n0          Rio Branco         1200401  2020-03-17                202012   \n1                 NaN              12  2020-03-17                202012   \n2          Rio Branco         1200401  2020-03-18                202012   \n3                 NaN              12  2020-03-18                202012   \n4          Rio Branco         1200401  2020-03-19                202012   \n...               ...             ...         ...                   ...   \n1736411      Tupirama         1721257  2021-03-15                202111   \n1736412    Tupiratins         1721307  2021-03-15                202111   \n1736413  Wanderlândia         1722081  2021-03-15                202111   \n1736414       Xambioá         1722107  2021-03-15                202111   \n1736415           NaN              17  2021-03-15                202111   \n\n         estimated_population  estimated_population_2019  is_last  \\\n0                    413418.0                   407319.0    False   \n1                    894470.0                   881935.0    False   \n2                    413418.0                   407319.0    False   \n3                    894470.0                   881935.0    False   \n4                    413418.0                   407319.0    False   \n...                       ...                        ...      ...   \n1736411                1922.0                     1891.0    False   \n1736412                2729.0                     2671.0    False   \n1736413               11734.0                    11683.0    False   \n1736414               11520.0                    11540.0    False   \n1736415             1590248.0                  1572866.0    False   \n\n         is_repeated  confirmed  \\\n0              False          3   \n1              False          3   \n2              False          3   \n3              False          3   \n4              False          4   \n...              ...        ...   \n1736411         True        120   \n1736412         True         51   \n1736413         True        623   \n1736414         True       1434   \n1736415         True     126131   \n\n         last_available_confirmed_per_100k_inhabitants last_available_date  \\\n0                                              0.72566          2020-03-17   \n1                                              0.33539          2020-03-17   \n2                                              0.72566          2020-03-18   \n3                                              0.33539          2020-03-18   \n4                                              0.96754          2020-03-19   \n...                                                ...                 ...   \n1736411                                     6243.49636          2021-03-13   \n1736412                                     1868.81642          2021-03-13   \n1736413                                     5309.35742          2021-03-13   \n1736414                                    12447.91667          2021-03-13   \n1736415                                     7931.53018          2021-03-14   \n\n         last_available_death_rate  deaths  order_for_place place_type state  \\\n0                           0.0000       0                1       city    AC   \n1                           0.0000       0                1      state    AC   \n2                           0.0000       0                2       city    AC   \n3                           0.0000       0                2      state    AC   \n4                           0.0000       0                3       city    AC   \n...                            ...     ...              ...        ...   ...   \n1736411                     0.0167       2              297       city    TO   \n1736412                     0.0196       1              297       city    TO   \n1736413                     0.0128       8              322       city    TO   \n1736414                     0.0126      18              311       city    TO   \n1736415                     0.0135    1697              363      state    TO   \n\n         new_confirmed  new_deaths  \n0                    3           0  \n1                    3           0  \n2                    0           0  \n3                    0           0  \n4                    1           0  \n...                ...         ...  \n1736411              0           0  \n1736412              0           0  \n1736413              0           0  \n1736414              0           0  \n1736415              0           0  \n\n[1729933 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"caso_full.csv\")\n",
    "df = df.rename(columns={\"last_available_confirmed\": \"confirmed\", \"last_available_deaths\": \"deaths\"})\n",
    "df = df[~df.city_ibge_code.isnull()]\n",
    "df = df.astype({\"city_ibge_code\": int})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def retrieve_data_for_all_cities(state):\n",
    "   # filter out states and imported cases\n",
    "   test = df[df.place_type.eq(\"city\")]\n",
    "   if (state == True):\n",
    "      test = test[test.state.eq(\"PR\")]\n",
    "   \n",
    "   all_codes = test[test.place_type.eq(\"city\")][\"city_ibge_code\"].unique()\n",
    "\n",
    "   test = test[[\"city_ibge_code\", \"date\", \"confirmed\", \"deaths\"]]\n",
    "   by_dates = [city for city in test.groupby('date')]\n",
    "\n",
    "   for i in range(len(by_dates)):\n",
    "      date, items = by_dates[i]\n",
    "\n",
    "      # del items[\"date\"]\n",
    "\n",
    "      # convert all ibge codes to a Series\n",
    "      pd_codes = pd.Series(all_codes)\n",
    "\n",
    "      # retrieve all cities which are not in items\n",
    "      not_in_list = pd_codes[~pd_codes.isin(items['city_ibge_code'])]\n",
    "\n",
    "      # create a new DataFrame with the missing cities. This is a lot faster than using pd.concat.\n",
    "      simple_list = []\n",
    "      for ibge in not_in_list:\n",
    "         simple_list.append([ibge, date, np.nan, np.nan])\n",
    "      \n",
    "      new_data = pd.DataFrame(simple_list, columns=['city_ibge_code', 'date', 'confirmed', 'deaths'])\n",
    "\n",
    "      # merge together both DataFrames\n",
    "      items = items.append(new_data, ignore_index=True)\n",
    "\n",
    "      # save back the values\n",
    "      by_dates[i] = [date, items]\n",
    "   return by_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve_data_fixed(state, steps=-7):\n",
    "  fixed_data = retrieve_data_for_all_cities(state)\n",
    "  \n",
    "  # add zero to first element. This will be propagated in the for loop.\n",
    "  fixed_data[0][1][['confirmed', 'deaths']] = fixed_data[0][1][['confirmed', 'deaths']].fillna(0)\n",
    "\n",
    "  for i in range(1, len(fixed_data)):\n",
    "    date, items = fixed_data[i]\n",
    "    prev_date, prev_items = fixed_data[i - 1]\n",
    "\n",
    "    # fill missing cities with previous value\n",
    "    # items['confirmed'] = items['confirmed'].fillna(prev_items['confirmed'])\n",
    "    items = items.fillna(prev_items)\n",
    "\n",
    "    # re-override the date column, since prev_items messed with it\n",
    "    items[\"date\"] = date\n",
    "\n",
    "    # fill remaining with zero\n",
    "    fixed_data[i] = [date, items]\n",
    "\n",
    "\n",
    "  smaller_date = []\n",
    "  for i in range(len(fixed_data) - 1, -1, steps):\n",
    "      date, items = fixed_data[i]\n",
    "      items = items.astype({\"confirmed\": int, \"deaths\": int})\n",
    "      items = items.sort_values(by='city_ibge_code', ascending=True)\n",
    "      smaller_date.append([date, items])\n",
    "  return smaller_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_csv(pr, name):\n",
    "    pr_df = retrieve_data_fixed(pr)\n",
    "    a = \"\"\n",
    "\n",
    "    for i in range(len(pr_df)):\n",
    "        date, items = pr_df[i]\n",
    "        items = items.rename(columns={\"city_ibge_code\": \"z\", \"confirmed\": \"c\", \"deaths\": \"d\"})\n",
    "        items = items[[\"date\", \"z\", \"c\", \"d\"]]\n",
    "        # if (not pr):\n",
    "            # limit the total number\n",
    "            # items = items.nlargest(3000, 'c')\n",
    "        a += items.to_csv(header= i==0, index=False)\n",
    "        pr_df[i] = [date, items]\n",
    "\n",
    "    with open(name, 'w') as outfile:\n",
    "        outfile.write(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_heatmap_csv(pr, name):\n",
    "    pr = retrieve_data_fixed(pr)\n",
    "\n",
    "    date, items = pr[0]\n",
    "    items = items.rename(columns={\"city_ibge_code\": \"z\", \"confirmed\": \"c\", \"deaths\": \"d\"})\n",
    "    items = items[[\"z\", \"c\"]]\n",
    "    items = items.to_csv(name, index = False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_csv(True, \"../public/data/pr_ndays.csv\")\n",
    "to_csv(False, \"../public/data/br_ndays.csv\")\n",
    "\n",
    "to_heatmap_csv(True, \"../public/data/pr_heatmap.csv\")\n",
    "to_heatmap_csv(False, \"../public/data/br_heatmap.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pr_df = retrieve_data_fixed(True, -1)\n",
    "# test = df[df.place_type.eq(\"city\")]\n",
    "# test = test[test.state.eq(\"PR\")]\n",
    "\n",
    "# top_pr_cities = test.sort_values('confirmed', ascending=False).drop_duplicates('city_ibge_code').head(8).sort_values('confirmed', ascending=False)['city_ibge_code']\n",
    "\n",
    "# by_dates = [city for city in test.groupby('date')]\n",
    "\n",
    "# a = \"\"\n",
    "# for i in range(len(by_dates)):\n",
    "#     date, items = pr_df[i]\n",
    "#     items = items.rename(columns={\"city_ibge_code\": \"z\", \"confirmed\": \"c\", \"deaths\": \"d\"})\n",
    "#     items = items[[\"date\", \"z\", \"c\", \"d\"]]\n",
    "#     items = items[items[\"z\"].isin(top_pr_cities)]\n",
    "\n",
    "#     a += items.to_csv(header= i==0, index=False)\n",
    "#     pr_df[i] = [date, items]\n",
    "\n",
    "# # with open(\"../public/data/pr_topcities_alldays.csv\", 'w') as outfile:\n",
    "# #     outfile.write(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}